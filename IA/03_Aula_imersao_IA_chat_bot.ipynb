{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/crislanecas/estudos_python/blob/main/IA/03_Aula_imersao_IA_chat_bot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conteúdo desenvolvido durante a **imersão IA** da Alura com o Google Gemini, de 06 a 10 de maio de 2024."
      ],
      "metadata": {
        "id": "_K9xmMeSE1MX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparando ambiente"
      ],
      "metadata": {
        "id": "xVcrYaiNXZtZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nFQLKQ6hfXch"
      },
      "outputs": [],
      "source": [
        "#Instala o SDK do Google Gemini\n",
        "!pip install -q -U google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importa as bibliotecas do Google\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "d4qMua5cWj6s"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configurando acesso"
      ],
      "metadata": {
        "id": "J9X4BCyYZFNq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IthzwfnAfcsi"
      },
      "outputs": [],
      "source": [
        "# Configura acesso via API key\n",
        "# google_api_key = 'sua_api_key'\n",
        "# genai.configure(api_key=google_api_key)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configura acesso via API key, utilizando userdata para ocultar a key no código\n",
        "api_key = userdata.get('secret_key_gemini')\n",
        "genai.configure(api_key=api_key)"
      ],
      "metadata": {
        "id": "tzdabnb5stEW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Verificando vesões"
      ],
      "metadata": {
        "id": "IJMbr46OIyys"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "id": "iD3ihugymWBt",
        "outputId": "9d00120b-5e64-47a2-e7e4-81c1bbf40b23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/gemini-1.0-pro\n",
            "models/gemini-1.0-pro-001\n",
            "models/gemini-1.0-pro-latest\n",
            "models/gemini-1.0-pro-vision-latest\n",
            "models/gemini-1.5-pro-latest\n",
            "models/gemini-pro\n",
            "models/gemini-pro-vision\n"
          ]
        }
      ],
      "source": [
        "# Verifica a lista dos modelos disponíveis para geração de conteúdo\n",
        "for m in genai.list_models():\n",
        "  if 'generateContent' in m.supported_generation_methods:\n",
        "    print(m.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configurando parâmetros\n",
        "\n",
        "Cria variáveis para serem usadas como parâmentros para o modelo."
      ],
      "metadata": {
        "id": "ZnFMLUM1IrKz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "O1ochaeyDn_o"
      },
      "outputs": [],
      "source": [
        "# Configura os parâmentros gerais do modelo\n",
        "generation_config = {\n",
        "  'candidate_count': 1, # Número de respostas\n",
        "  'temperature': 0.5, # Variabilidade da criatividade de 0 a 1\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "jwBv7DofDofw"
      },
      "outputs": [],
      "source": [
        "# Configura os parâmentros de segurança do modelo\n",
        "safety_settings = {\n",
        "    'HATE': 'BLOCK_NONE',\n",
        "    'HARASSMENT': 'BLOCK_NONE',\n",
        "    'SEXUAL': 'BLOCK_NONE',\n",
        "    'DANGEROUS': 'BLOCK_NONE'\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configurando o modelo"
      ],
      "metadata": {
        "id": "BixRyYdkaO1X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "NIiSs1R5mVJE"
      },
      "outputs": [],
      "source": [
        "# Configura o modelo e informando os argumentos especificados acima\n",
        "model = genai.GenerativeModel(model_name='gemini-1.0-pro',\n",
        "                                  generation_config=generation_config,\n",
        "                                  safety_settings=safety_settings,)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configurando respostas"
      ],
      "metadata": {
        "id": "f9Oot97Sb_rd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "vrRjhT2rlrU_",
        "outputId": "5744494f-0c98-4be2-98f6-e6a9531abddd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Google'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# Recebe a resposta do modelo\n",
        "response = model.generate_content('Que empresa criou o modelo de IA Gemini?')\n",
        "response.text # Seleciona apenas o atributo 'text' para a resposta"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Criando Chat Bot"
      ],
      "metadata": {
        "id": "ceNf8ZUveL94"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KpaQ6hm5f2_J",
        "outputId": "5e3e0209-065f-4880-8769-6e148438a766"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Digiti o prompt: Qual a diferença entre Spark e PySpark?\n",
            "Resposta: **Apache Spark** e **PySpark** são dois conceitos diferentes, embora relacionados:\n",
            "\n",
            "**Apache Spark**\n",
            "\n",
            "* Um mecanismo de processamento distribuído de código aberto para processamento de dados em grande escala.\n",
            "* Fornece uma API unificada para processamento em lote, processamento de streaming e aprendizado de máquina.\n",
            "* Pode ser usado em vários idiomas, incluindo Python, Java e Scala.\n",
            "\n",
            "**PySpark**\n",
            "\n",
            "* Uma interface Python para o Apache Spark.\n",
            "* Permite que os desenvolvedores do Python usem as funcionalidades do Spark de forma conveniente.\n",
            "* Fornece uma API Python para criar e manipular conjuntos de dados Spark, executar transformações e ações e interagir com outros componentes do Spark.\n",
            "\n",
            "**Diferenças:**\n",
            "\n",
            "* **Linguagem:** O Apache Spark é uma estrutura multi-linguagem, enquanto o PySpark é uma interface Python específica.\n",
            "* **Nível de abstração:** O PySpark oferece um nível de abstração mais alto, tornando mais fácil para os desenvolvedores do Python trabalhar com o Spark.\n",
            "* **Desempenho:** O Apache Spark geralmente oferece melhor desempenho do que o PySpark, pois é implementado em Java e Scala, que são mais eficientes para processamento de dados.\n",
            "* **Facilidade de uso:** O PySpark é mais fácil de usar para desenvolvedores do Python, pois fornece uma API mais intuitiva e familiar.\n",
            "\n",
            "**Resumo:**\n",
            "\n",
            "O Apache Spark é o mecanismo subjacente que fornece recursos de processamento de dados, enquanto o PySpark é uma interface Python que facilita o uso do Spark para desenvolvedores do Python. A escolha entre os dois depende das necessidades específicas do projeto e do nível de experiência do desenvolvedor. \n",
            "\n",
            "\n",
            "Digite FIM para encerrar a conversa.\n",
            "Digiti um novo prompt: Como configurar o Colab para usar PySpark?\n",
            "Resposta: Para configurar o Colab para usar o PySpark, siga estas etapas:\n",
            "\n",
            "1. **Crie um novo notebook do Colab.**\n",
            "2. **Clique em \"Editar\" > \"Configurações de tempo de execução\".**\n",
            "3. **Na seção \"Tipo de tempo de execução\", selecione \"Python 3\".**\n",
            "4. **Na seção \"Configurações de hardware\", selecione uma GPU ou TPU, se disponível.**\n",
            "5. **Clique em \"Salvar\".**\n",
            "6. **Reinicie o tempo de execução clicando no botão \"Reiniciar tempo de execução\" na barra de ferramentas.**\n",
            "\n",
            "Depois de reiniciar o tempo de execução, você pode instalar o PySpark usando o seguinte comando:\n",
            "\n",
            "```\n",
            "!pip install pyspark\n",
            "```\n",
            "\n",
            "Para verificar se o PySpark foi instalado com sucesso, execute o seguinte comando:\n",
            "\n",
            "```\n",
            "import pyspark\n",
            "pyspark.version.__version__\n",
            "```\n",
            "\n",
            "Se o comando retornar uma versão do PySpark, a instalação foi bem-sucedida.\n",
            "\n",
            "**Observações:**\n",
            "\n",
            "* O Colab oferece suporte apenas para versões específicas do PySpark. Verifique a documentação do Colab para obter a versão mais recente compatível.\n",
            "* Você pode precisar instalar o Java Development Kit (JDK) se ainda não o tiver instalado.\n",
            "* Se você estiver usando uma GPU ou TPU, poderá precisar instalar bibliotecas adicionais para suporte ao PySpark.\n",
            "\n",
            "**Exemplo de uso:**\n",
            "\n",
            "Depois de instalar o PySpark, você pode usá-lo para criar um DataFrame do Spark:\n",
            "\n",
            "```\n",
            "from pyspark.sql import SparkSession\n",
            "\n",
            "spark = SparkSession.builder.appName(\"Colab PySpark\").getOrCreate()\n",
            "\n",
            "df = spark.createDataFrame([(1, \"John\"), (2, \"Jane\"), (3, \"Bob\")], [\"id\", \"name\"])\n",
            "\n",
            "df.show()\n",
            "```\n",
            "\n",
            "Isso criará e exibirá um DataFrame do Spark com três linhas e duas colunas. \n",
            "\n",
            "\n",
            "Digite FIM para encerrar a conversa.\n",
            "Digiti um novo prompt: fim\n"
          ]
        }
      ],
      "source": [
        "# Recebe uma lista com histórico\n",
        "chat = model.start_chat(history=[])\n",
        "\n",
        "# Recebe o prompt de entrada\n",
        "prompt = input('Digiti o prompt: ')\n",
        "\n",
        "# Cria um loop para receber o prompt e enviar para o modelo\n",
        "while prompt != 'fim': # Define qualquer input diferente de 'fim' para execução\n",
        "  response = chat.send_message(prompt)\n",
        "  print('Resposta:', response.text, '\\n\\n')\n",
        "  print('Digite FIM para encerrar a conversa.')\n",
        "  prompt = input('Digiti um novo prompt: ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "XLAniTTDhHNW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "abe38f92-74d1-47d0-cdc1-1363bb51902e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatSession(\n",
              "    model=genai.GenerativeModel(\n",
              "        model_name='models/gemini-1.0-pro',\n",
              "        generation_config={'candidate_count': 1, 'temperature': 0.5},\n",
              "        safety_settings={<HarmCategory.HARM_CATEGORY_HATE_SPEECH: 8>: <HarmBlockThreshold.BLOCK_NONE: 4>, <HarmCategory.HARM_CATEGORY_HARASSMENT: 7>: <HarmBlockThreshold.BLOCK_NONE: 4>, <HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: 9>: <HarmBlockThreshold.BLOCK_NONE: 4>, <HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: 10>: <HarmBlockThreshold.BLOCK_NONE: 4>},\n",
              "        tools=None,\n",
              "        system_instruction=None,\n",
              "    ),\n",
              "    history=[glm.Content({'parts': [{'text': 'Qual a difer...rk e PySpark?'}], 'role': 'user'}), glm.Content({'parts': [{'text': '**Apache Spa...esenvolvedor.'}], 'role': 'model'}), glm.Content({'parts': [{'text': 'Como configu...usar PySpark?'}], 'role': 'user'}), glm.Content({'parts': [{'text': 'Para configu...duas colunas.'}], 'role': 'model'})]\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# Exibe o conteúdo da variável\n",
        "chat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "sd8mvW9KghTf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "ec68e6dd-f791-4f2e-e62e-fbe2fda245ff"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[parts {\n",
              "   text: \"Qual a diferen\\303\\247a entre Spark e PySpark?\"\n",
              " }\n",
              " role: \"user\",\n",
              " parts {\n",
              "   text: \"**Apache Spark** e **PySpark** s\\303\\243o dois conceitos diferentes, embora relacionados:\\n\\n**Apache Spark**\\n\\n* Um mecanismo de processamento distribu\\303\\255do de c\\303\\263digo aberto para processamento de dados em grande escala.\\n* Fornece uma API unificada para processamento em lote, processamento de streaming e aprendizado de m\\303\\241quina.\\n* Pode ser usado em v\\303\\241rios idiomas, incluindo Python, Java e Scala.\\n\\n**PySpark**\\n\\n* Uma interface Python para o Apache Spark.\\n* Permite que os desenvolvedores do Python usem as funcionalidades do Spark de forma conveniente.\\n* Fornece uma API Python para criar e manipular conjuntos de dados Spark, executar transforma\\303\\247\\303\\265es e a\\303\\247\\303\\265es e interagir com outros componentes do Spark.\\n\\n**Diferen\\303\\247as:**\\n\\n* **Linguagem:** O Apache Spark \\303\\251 uma estrutura multi-linguagem, enquanto o PySpark \\303\\251 uma interface Python espec\\303\\255fica.\\n* **N\\303\\255vel de abstra\\303\\247\\303\\243o:** O PySpark oferece um n\\303\\255vel de abstra\\303\\247\\303\\243o mais alto, tornando mais f\\303\\241cil para os desenvolvedores do Python trabalhar com o Spark.\\n* **Desempenho:** O Apache Spark geralmente oferece melhor desempenho do que o PySpark, pois \\303\\251 implementado em Java e Scala, que s\\303\\243o mais eficientes para processamento de dados.\\n* **Facilidade de uso:** O PySpark \\303\\251 mais f\\303\\241cil de usar para desenvolvedores do Python, pois fornece uma API mais intuitiva e familiar.\\n\\n**Resumo:**\\n\\nO Apache Spark \\303\\251 o mecanismo subjacente que fornece recursos de processamento de dados, enquanto o PySpark \\303\\251 uma interface Python que facilita o uso do Spark para desenvolvedores do Python. A escolha entre os dois depende das necessidades espec\\303\\255ficas do projeto e do n\\303\\255vel de experi\\303\\252ncia do desenvolvedor.\"\n",
              " }\n",
              " role: \"model\",\n",
              " parts {\n",
              "   text: \"Como configurar o Colab para usar PySpark?\"\n",
              " }\n",
              " role: \"user\",\n",
              " parts {\n",
              "   text: \"Para configurar o Colab para usar o PySpark, siga estas etapas:\\n\\n1. **Crie um novo notebook do Colab.**\\n2. **Clique em \\\"Editar\\\" > \\\"Configura\\303\\247\\303\\265es de tempo de execu\\303\\247\\303\\243o\\\".**\\n3. **Na se\\303\\247\\303\\243o \\\"Tipo de tempo de execu\\303\\247\\303\\243o\\\", selecione \\\"Python 3\\\".**\\n4. **Na se\\303\\247\\303\\243o \\\"Configura\\303\\247\\303\\265es de hardware\\\", selecione uma GPU ou TPU, se dispon\\303\\255vel.**\\n5. **Clique em \\\"Salvar\\\".**\\n6. **Reinicie o tempo de execu\\303\\247\\303\\243o clicando no bot\\303\\243o \\\"Reiniciar tempo de execu\\303\\247\\303\\243o\\\" na barra de ferramentas.**\\n\\nDepois de reiniciar o tempo de execu\\303\\247\\303\\243o, voc\\303\\252 pode instalar o PySpark usando o seguinte comando:\\n\\n```\\n!pip install pyspark\\n```\\n\\nPara verificar se o PySpark foi instalado com sucesso, execute o seguinte comando:\\n\\n```\\nimport pyspark\\npyspark.version.__version__\\n```\\n\\nSe o comando retornar uma vers\\303\\243o do PySpark, a instala\\303\\247\\303\\243o foi bem-sucedida.\\n\\n**Observa\\303\\247\\303\\265es:**\\n\\n* O Colab oferece suporte apenas para vers\\303\\265es espec\\303\\255ficas do PySpark. Verifique a documenta\\303\\247\\303\\243o do Colab para obter a vers\\303\\243o mais recente compat\\303\\255vel.\\n* Voc\\303\\252 pode precisar instalar o Java Development Kit (JDK) se ainda n\\303\\243o o tiver instalado.\\n* Se voc\\303\\252 estiver usando uma GPU ou TPU, poder\\303\\241 precisar instalar bibliotecas adicionais para suporte ao PySpark.\\n\\n**Exemplo de uso:**\\n\\nDepois de instalar o PySpark, voc\\303\\252 pode us\\303\\241-lo para criar um DataFrame do Spark:\\n\\n```\\nfrom pyspark.sql import SparkSession\\n\\nspark = SparkSession.builder.appName(\\\"Colab PySpark\\\").getOrCreate()\\n\\ndf = spark.createDataFrame([(1, \\\"John\\\"), (2, \\\"Jane\\\"), (3, \\\"Bob\\\")], [\\\"id\\\", \\\"name\\\"])\\n\\ndf.show()\\n```\\n\\nIsso criar\\303\\241 e exibir\\303\\241 um DataFrame do Spark com tr\\303\\252s linhas e duas colunas.\"\n",
              " }\n",
              " role: \"model\"]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# Exibe o conteúdo da variável\n",
        "chat.history"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imprimindo histórico formatado"
      ],
      "metadata": {
        "id": "qlb0SBOciCaM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "dAcbPBocgzeX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d1b95cf5-7078-4476-acea-fa3433a63220"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> **user**: Qual a diferença entre Spark e PySpark?"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> **model**: **Apache Spark** e **PySpark** são dois conceitos diferentes, embora relacionados:\n> \n> **Apache Spark**\n> \n> * Um mecanismo de processamento distribuído de código aberto para processamento de dados em grande escala.\n> * Fornece uma API unificada para processamento em lote, processamento de streaming e aprendizado de máquina.\n> * Pode ser usado em vários idiomas, incluindo Python, Java e Scala.\n> \n> **PySpark**\n> \n> * Uma interface Python para o Apache Spark.\n> * Permite que os desenvolvedores do Python usem as funcionalidades do Spark de forma conveniente.\n> * Fornece uma API Python para criar e manipular conjuntos de dados Spark, executar transformações e ações e interagir com outros componentes do Spark.\n> \n> **Diferenças:**\n> \n> * **Linguagem:** O Apache Spark é uma estrutura multi-linguagem, enquanto o PySpark é uma interface Python específica.\n> * **Nível de abstração:** O PySpark oferece um nível de abstração mais alto, tornando mais fácil para os desenvolvedores do Python trabalhar com o Spark.\n> * **Desempenho:** O Apache Spark geralmente oferece melhor desempenho do que o PySpark, pois é implementado em Java e Scala, que são mais eficientes para processamento de dados.\n> * **Facilidade de uso:** O PySpark é mais fácil de usar para desenvolvedores do Python, pois fornece uma API mais intuitiva e familiar.\n> \n> **Resumo:**\n> \n> O Apache Spark é o mecanismo subjacente que fornece recursos de processamento de dados, enquanto o PySpark é uma interface Python que facilita o uso do Spark para desenvolvedores do Python. A escolha entre os dois depende das necessidades específicas do projeto e do nível de experiência do desenvolvedor."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> **user**: Como configurar o Colab para usar PySpark?"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> **model**: Para configurar o Colab para usar o PySpark, siga estas etapas:\n> \n> 1. **Crie um novo notebook do Colab.**\n> 2. **Clique em \"Editar\" > \"Configurações de tempo de execução\".**\n> 3. **Na seção \"Tipo de tempo de execução\", selecione \"Python 3\".**\n> 4. **Na seção \"Configurações de hardware\", selecione uma GPU ou TPU, se disponível.**\n> 5. **Clique em \"Salvar\".**\n> 6. **Reinicie o tempo de execução clicando no botão \"Reiniciar tempo de execução\" na barra de ferramentas.**\n> \n> Depois de reiniciar o tempo de execução, você pode instalar o PySpark usando o seguinte comando:\n> \n> ```\n> !pip install pyspark\n> ```\n> \n> Para verificar se o PySpark foi instalado com sucesso, execute o seguinte comando:\n> \n> ```\n> import pyspark\n> pyspark.version.__version__\n> ```\n> \n> Se o comando retornar uma versão do PySpark, a instalação foi bem-sucedida.\n> \n> **Observações:**\n> \n> * O Colab oferece suporte apenas para versões específicas do PySpark. Verifique a documentação do Colab para obter a versão mais recente compatível.\n> * Você pode precisar instalar o Java Development Kit (JDK) se ainda não o tiver instalado.\n> * Se você estiver usando uma GPU ou TPU, poderá precisar instalar bibliotecas adicionais para suporte ao PySpark.\n> \n> **Exemplo de uso:**\n> \n> Depois de instalar o PySpark, você pode usá-lo para criar um DataFrame do Spark:\n> \n> ```\n> from pyspark.sql import SparkSession\n> \n> spark = SparkSession.builder.appName(\"Colab PySpark\").getOrCreate()\n> \n> df = spark.createDataFrame([(1, \"John\"), (2, \"Jane\"), (3, \"Bob\")], [\"id\", \"name\"])\n> \n> df.show()\n> ```\n> \n> Isso criará e exibirá um DataFrame do Spark com três linhas e duas colunas."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Para formatação de texto, especificamente para 'quebrar' linhas\n",
        "import textwrap\n",
        "# Para exibir diversos tipos de conteúdo dentro de notebooks\n",
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "\n",
        "# Cria uma função para formar em Markdow a resposta do modelo\n",
        "def to_markdown(text):\n",
        "  text = text.replace('•', '  *') # Substitui marcadores ' *' no texto de entrada\n",
        "  # Aplica um recuo a cada linha do texto usando o caractere '> ', criando o efeito de citação típico do Markdown\n",
        "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True)) # A função predicate=lambda _: True garante que todas as linhas sejam recuadas\n",
        "\n",
        "# Cria um loop para percorre cada mensagem no histórico\n",
        "for message in chat.history:\n",
        "  display(to_markdown(f'**{message.role}**: {message.parts[0].text}')) # Formata a mensagem para exibição\n",
        "  print('-------------------------------------------')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fonte\n",
        "\n",
        "https://www.alura.com.br/imersao-ia-google-gemini\n",
        "\n",
        "[Guia de início rápido da API Gemini](https://ai.google.dev/gemini-api/docs/quickstart?hl=pt-br)\n",
        "\n",
        "[Colab modelo - Google](https://colab.research.google.com/github/google/generative-ai-docs/blob/main/site/en/tutorials/quickstart_colab.ipynb?hl=pt-br)\n",
        "\n",
        "[Colab da aula - Imersão IA 2° Edição | 2024](https://colab.research.google.com/drive/1wFIctGOaYwlgXD8xsyBiSi5LmTHrObai?usp=sharing#scrollTo=KpaQ6hm5f2_J)"
      ],
      "metadata": {
        "id": "xaZClMudc_8b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Glossário\n",
        "\n",
        "**SDK** (Software Development Kit): conjunto de ferramentas utilizado para integrar os recursos do modelo de linguagem Gemini."
      ],
      "metadata": {
        "id": "EeD35iULSGOL"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}